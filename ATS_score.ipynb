{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nishtha Singla\\Downloads\\ATS_score_checker_using_Gemini_API-main (1)\\ATS_score_checker_using_Gemini_API-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Nishtha\n",
      "[nltk_data]     Singla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nishtha\n",
      "[nltk_data]     Singla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Nishtha\n",
      "[nltk_data]     Singla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gradio as gr\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = ''\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + '\\n'\n",
    "    return text\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(job_description):\n",
    "    tokens = preprocess_text(job_description)\n",
    "    return set(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate ATS Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(resume_text, job_keywords):\n",
    "    resume_tokens = preprocess_text(resume_text)\n",
    "    # Count unique matches (each keyword in job_keywords is counted only once)\n",
    "    match_count = sum(1 for keyword in job_keywords if keyword in resume_tokens)\n",
    "    if len(job_keywords) > 0:\n",
    "        score = (match_count / len(job_keywords)) * 100\n",
    "    elif score > 100:\n",
    "        score = 100\n",
    "    else:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATS Scanner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ats_scanner(resume_path, job_description):\n",
    "    resume_text = extract_text(resume_path)\n",
    "    job_keywords = extract_keywords(job_description)\n",
    "    score = calculate_score(resume_text, job_keywords)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Default Jobs and Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_jobs = {\n",
    "    \"Python Developer\": \"\"\"\n",
    "    We are looking for a Python developer with experience in data analysis, machine learning, and web development.\n",
    "    Skills required: Python, Pandas, NumPy, Flask, Django, SQL.\n",
    "    \"\"\",\n",
    "    \"Data Scientist\": \"\"\"\n",
    "    We are hiring a Data Scientist with expertise in machine learning, statistical analysis, and data visualization.\n",
    "    Skills required: Python, R, TensorFlow, Scikit-learn, SQL, Tableau.\n",
    "    \"\"\",\n",
    "    \"Frontend Developer\": \"\"\"\n",
    "    We need a Frontend Developer proficient in HTML, CSS, JavaScript, and modern frameworks like React or Angular.\n",
    "    Skills required: HTML, CSS, JavaScript, React, Angular, Git.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "def ats_scanner_interface(resume_file, selected_job, custom_job_description):\n",
    "    # Use the selected job description or custom job description\n",
    "    if selected_job != \"Custom\":\n",
    "        job_description = default_jobs[selected_job]\n",
    "    else:\n",
    "        job_description = custom_job_description\n",
    "    \n",
    "    # Get the temporary file path from the uploaded file\n",
    "    resume_path = resume_file.name\n",
    "    \n",
    "    # Calculate ATS score\n",
    "    score = ats_scanner(resume_path, job_description)\n",
    "    return f\"ATS Score: {score:.2f}%\"\n",
    "\n",
    "# Create Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ats_scanner_interface,\n",
    "    inputs=[\n",
    "        gr.components.File(label=\"Upload Resume (PDF or DOCX)\"),\n",
    "        gr.components.Dropdown(choices=list(default_jobs.keys()) + [\"Custom\"], label=\"Select Job\"),\n",
    "        gr.components.Textbox(label=\"Custom Job Description (if 'Custom' is selected)\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"ATS Resume Scanner\",\n",
    "    description=\"Upload your resume, select a job, or enter a custom job description to get an ATS score.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feedback and gpt2 integartion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Nishtha\n",
      "[nltk_data]     Singla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nishtha\n",
      "[nltk_data]     Singla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set your Google Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyCcXmmz8ZYl3uAXu8o0y2tq_Seup1OJ0zQ\")\n",
    "\n",
    "# Resume parsing functions\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or ''\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Keyword extraction\n",
    "def extract_keywords(job_description):\n",
    "    tokens = preprocess_text(job_description)\n",
    "    return set(tokens)\n",
    "\n",
    "# Calculate ATS score\n",
    "def calculate_score(resume_text, job_keywords):\n",
    "    resume_tokens = preprocess_text(resume_text)\n",
    "    match_count = sum(1 for keyword in job_keywords if keyword in resume_tokens)\n",
    "    return (match_count / len(job_keywords)) * 100 if job_keywords else 0\n",
    "\n",
    "# ATS scanner function\n",
    "def ats_scanner(resume_path, job_description):\n",
    "    resume_text = extract_text(resume_path)\n",
    "    job_keywords = extract_keywords(job_description)\n",
    "    score = calculate_score(resume_text, job_keywords)\n",
    "    return score, resume_text\n",
    "\n",
    "# AI feedback function using Gemini\n",
    "def get_ai_feedback(resume_text, job_description, score):\n",
    "    prompt = f\"\"\"\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    Resume Text:\n",
    "    {resume_text}\n",
    "\n",
    "    The resume received an ATS score of {score:.2f}% for this job. Please provide feedback on why the score is what it is and suggest specific improvements to increase the score. Focus on missing keywords, skills, or experiences that are relevant to the job description.\n",
    "    Format the output using HTML to highlight important parts instead of using Markdown-style bold (**).\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')  \n",
    "    response = model.generate_content(prompt)\n",
    "    feedback = response.text\n",
    "\n",
    "    # Replace Markdown bold **text** with HTML <mark>text</mark>\n",
    "    feedback = re.sub(r'\\*\\*(.*?)\\*\\*', r'<mark>\\1</mark>', feedback)\n",
    "\n",
    "    return feedback\n",
    "\n",
    "# Gradio interface\n",
    "def ats_scanner_interface(resume_file, selected_job, custom_job_description):\n",
    "    if selected_job != \"Custom\":\n",
    "        job_description = default_jobs[selected_job]\n",
    "    else:\n",
    "        job_description = custom_job_description\n",
    "    resume_path = resume_file.name\n",
    "    score, resume_text = ats_scanner(resume_path, job_description)\n",
    "    feedback = get_ai_feedback(resume_text, job_description, score)\n",
    "    return f\"ATS Score: {score:.2f}%\", feedback\n",
    "\n",
    "# Default jobs and descriptions\n",
    "default_jobs = {\n",
    "    \"Python Developer\": \"\"\"\n",
    "    We are looking for a Python developer with experience in data analysis, machine learning, and web development.\n",
    "    Skills required: Python, Pandas, NumPy, Flask, Django, SQL.\n",
    "    \"\"\",\n",
    "    \"Data Scientist\": \"\"\"\n",
    "    We are hiring a Data Scientist with expertise in machine learning, statistical analysis, and data visualization.\n",
    "    Skills required: Python, R, TensorFlow, Scikit-learn, SQL, Tableau.\n",
    "    \"\"\",\n",
    "    \"Frontend Developer\": \"\"\"\n",
    "    We need a Frontend Developer proficient in HTML, CSS, JavaScript, and modern frameworks like React or Angular.\n",
    "    Skills required: HTML, CSS, JavaScript, React, Angular, Git.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Create Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ats_scanner_interface,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Resume (PDF or DOCX)\"),\n",
    "        gr.Dropdown(choices=list(default_jobs.keys()) + [\"Custom\"], label=\"Select Job\"),\n",
    "        gr.Textbox(label=\"Custom Job Description (if 'Custom' is selected)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"ATS Score\"),\n",
    "        gr.HTML(label=\"AI Feedback\")  # Updated to HTML for formatted output\n",
    "    ],\n",
    "    title=\"AI-Powered ATS Resume Scanner (Gemini)\",\n",
    "    description=\"Upload your resume, select a job, or enter a custom job description to get an ATS score and AI feedback using Google Gemini.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=\"AIzaSyCcXmmz8ZYl3uAXu8o0y2tq_Seup1OJ0zQ\")\n",
    "\n",
    "# List available models\n",
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
